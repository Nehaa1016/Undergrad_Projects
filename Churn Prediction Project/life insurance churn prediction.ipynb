{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff118cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\neha\\anaconda3\\lib\\site-packages (2.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a4c4c",
   "metadata": {},
   "source": [
    "# DATA CLEANING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0303b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('randomdata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e43bb",
   "metadata": {},
   "source": [
    "# #Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3c9b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Customer_Address</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Claim Reason</th>\n",
       "      <th>Data confidentiality</th>\n",
       "      <th>Claim Amount</th>\n",
       "      <th>Category Premium</th>\n",
       "      <th>Premium/Amount Ratio</th>\n",
       "      <th>Claim Request output</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Christine Payne</td>\n",
       "      <td>7627 Anderson Rest Apt. 265,Lake Heather, DC 3...</td>\n",
       "      <td>Williams, Henderson and Perez</td>\n",
       "      <td>Travel</td>\n",
       "      <td>Low</td>\n",
       "      <td>377</td>\n",
       "      <td>4794</td>\n",
       "      <td>0.078640</td>\n",
       "      <td>No</td>\n",
       "      <td>21</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Tony Fernandez</td>\n",
       "      <td>3953 Cindy Brook Apt. 147,East Lindatown, TN 4...</td>\n",
       "      <td>Moore-Goodwin</td>\n",
       "      <td>Medical</td>\n",
       "      <td>High</td>\n",
       "      <td>1440</td>\n",
       "      <td>14390</td>\n",
       "      <td>0.100069</td>\n",
       "      <td>No</td>\n",
       "      <td>24</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Christopher Kim</td>\n",
       "      <td>8693 Walters Mountains,South Tony, TX 88407</td>\n",
       "      <td>Smith-Holmes</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Medium</td>\n",
       "      <td>256</td>\n",
       "      <td>1875</td>\n",
       "      <td>0.136533</td>\n",
       "      <td>No</td>\n",
       "      <td>18</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Nicole Allen</td>\n",
       "      <td>56926 Webster Coves,Shawnmouth, NV 04853</td>\n",
       "      <td>Harrell-Perez</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Medium</td>\n",
       "      <td>233</td>\n",
       "      <td>1875</td>\n",
       "      <td>0.124267</td>\n",
       "      <td>No</td>\n",
       "      <td>24</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Linda Cruz</td>\n",
       "      <td>489 Thomas Forges Apt. 305,Jesseton, GA 36765</td>\n",
       "      <td>Simpson, Kramer and Hughes</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Medium</td>\n",
       "      <td>239</td>\n",
       "      <td>1875</td>\n",
       "      <td>0.127467</td>\n",
       "      <td>No</td>\n",
       "      <td>21</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Customer Name  \\\n",
       "0           0  Christine Payne   \n",
       "1           1   Tony Fernandez   \n",
       "2           2  Christopher Kim   \n",
       "3           3     Nicole Allen   \n",
       "4           4       Linda Cruz   \n",
       "\n",
       "                                    Customer_Address  \\\n",
       "0  7627 Anderson Rest Apt. 265,Lake Heather, DC 3...   \n",
       "1  3953 Cindy Brook Apt. 147,East Lindatown, TN 4...   \n",
       "2        8693 Walters Mountains,South Tony, TX 88407   \n",
       "3           56926 Webster Coves,Shawnmouth, NV 04853   \n",
       "4      489 Thomas Forges Apt. 305,Jesseton, GA 36765   \n",
       "\n",
       "                    Company Name Claim Reason Data confidentiality  \\\n",
       "0  Williams, Henderson and Perez       Travel                  Low   \n",
       "1                  Moore-Goodwin      Medical                 High   \n",
       "2                   Smith-Holmes        Phone               Medium   \n",
       "3                  Harrell-Perez        Phone               Medium   \n",
       "4     Simpson, Kramer and Hughes        Phone               Medium   \n",
       "\n",
       "   Claim Amount  Category Premium  Premium/Amount Ratio Claim Request output  \\\n",
       "0           377              4794              0.078640                   No   \n",
       "1          1440             14390              0.100069                   No   \n",
       "2           256              1875              0.136533                   No   \n",
       "3           233              1875              0.124267                   No   \n",
       "4           239              1875              0.127467                   No   \n",
       "\n",
       "   BMI Churn  \n",
       "0   21   Yes  \n",
       "1   24   Yes  \n",
       "2   18   Yes  \n",
       "3   24   Yes  \n",
       "4   21   Yes  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2725d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   Unnamed: 0            200000 non-null  int64  \n",
      " 1   Customer Name         200000 non-null  object \n",
      " 2   Customer_Address      200000 non-null  object \n",
      " 3   Company Name          200000 non-null  object \n",
      " 4   Claim Reason          200000 non-null  object \n",
      " 5   Data confidentiality  200000 non-null  object \n",
      " 6   Claim Amount          200000 non-null  int64  \n",
      " 7   Category Premium      200000 non-null  int64  \n",
      " 8   Premium/Amount Ratio  200000 non-null  float64\n",
      " 9   Claim Request output  200000 non-null  object \n",
      " 10  BMI                   200000 non-null  int64  \n",
      " 11  Churn                 200000 non-null  object \n",
      "dtypes: float64(1), int64(4), object(7)\n",
      "memory usage: 18.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45498a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Claim Amount</th>\n",
       "      <th>Category Premium</th>\n",
       "      <th>Premium/Amount Ratio</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>99999.500000</td>\n",
       "      <td>1120.478840</td>\n",
       "      <td>8963.783895</td>\n",
       "      <td>0.125024</td>\n",
       "      <td>23.007205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57735.171256</td>\n",
       "      <td>796.660796</td>\n",
       "      <td>6114.737202</td>\n",
       "      <td>0.034742</td>\n",
       "      <td>3.164976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49999.750000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>1875.000000</td>\n",
       "      <td>0.106741</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>99999.500000</td>\n",
       "      <td>1390.000000</td>\n",
       "      <td>14390.000000</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>149999.250000</td>\n",
       "      <td>1844.000000</td>\n",
       "      <td>14390.000000</td>\n",
       "      <td>0.143155</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>199999.000000</td>\n",
       "      <td>2299.000000</td>\n",
       "      <td>14390.000000</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0   Claim Amount  Category Premium  Premium/Amount Ratio  \\\n",
       "count  200000.000000  200000.000000     200000.000000         200000.000000   \n",
       "mean    99999.500000    1120.478840       8963.783895              0.125024   \n",
       "std     57735.171256     796.660796       6114.737202              0.034742   \n",
       "min         0.000000       1.000000        399.000000              0.002506   \n",
       "25%     49999.750000     245.000000       1875.000000              0.106741   \n",
       "50%     99999.500000    1390.000000      14390.000000              0.125122   \n",
       "75%    149999.250000    1844.000000      14390.000000              0.143155   \n",
       "max    199999.000000    2299.000000      14390.000000              0.248120   \n",
       "\n",
       "                 BMI  \n",
       "count  200000.000000  \n",
       "mean       23.007205  \n",
       "std         3.164976  \n",
       "min        18.000000  \n",
       "25%        20.000000  \n",
       "50%        23.000000  \n",
       "75%        26.000000  \n",
       "max        28.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924c9a3",
   "metadata": {},
   "source": [
    "# # Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3973390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove Duplicate Rows\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17849db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Remove Irrelevant Columns\n",
    "# Identify and drop columns that are not relevant for churn prediction\n",
    "irrelevant_columns = ['Customer Name', 'Customer_Address', 'Company Name', 'Data confidentiality', 'Claim Amount', 'Category Premium', 'Premium/Amount Ratio']\n",
    "data.drop(columns=irrelevant_columns, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004a69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preprocessing\n",
    "# After removing duplicates and irrelevant columns, you may proceed with data preprocessing\n",
    "# This may include handling missing data, encoding categorical variables, and scaling/normalizing numerical features\n",
    "\n",
    "# Handling Missing Data\n",
    "data.dropna(subset=['Churn'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0cac9a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0    Customer Name  \\\n",
      "0                0  Christine Payne   \n",
      "1                1   Tony Fernandez   \n",
      "2                2  Christopher Kim   \n",
      "3                3     Nicole Allen   \n",
      "4                4       Linda Cruz   \n",
      "...            ...              ...   \n",
      "199995      199995  Matthew Estrada   \n",
      "199996      199996       James Bean   \n",
      "199997      199997      David Meyer   \n",
      "199998      199998     Martha Stone   \n",
      "199999      199999    Shannon Lewis   \n",
      "\n",
      "                                         Customer_Address  Company Name  \\\n",
      "0       7627 Anderson Rest Apt. 265,Lake Heather, DC 3...        122584   \n",
      "1       3953 Cindy Brook Apt. 147,East Lindatown, TN 4...         77347   \n",
      "2             8693 Walters Mountains,South Tony, TX 88407        106968   \n",
      "3                56926 Webster Coves,Shawnmouth, NV 04853         44952   \n",
      "4           489 Thomas Forges Apt. 305,Jesseton, GA 36765        104639   \n",
      "...                                                   ...           ...   \n",
      "199995       2024 Lopez Gateway,Lake Pamelafort, MS 35772         16350   \n",
      "199996             0268 Lori Falls,West Jeffrey, SC 49142        114158   \n",
      "199997         00573 Miller Cliff,New Allenbury, SC 68902        104547   \n",
      "199998          62681 Peters Cove,South Anthony, RI 99783          4850   \n",
      "199999                    Unit 6569 Box 2236,DPO AE 88045         98869   \n",
      "\n",
      "        Claim Reason Data confidentiality  Claim Amount  Category Premium  \\\n",
      "0                  3                  Low           377                 2   \n",
      "1                  0                 High          1440                 3   \n",
      "2                  2               Medium           256                 1   \n",
      "3                  2               Medium           233                 1   \n",
      "4                  2               Medium           239                 1   \n",
      "...              ...                  ...           ...               ...   \n",
      "199995             0                 High          1563                 3   \n",
      "199996             0                 High          1342                 3   \n",
      "199997             0                 High          2278                 3   \n",
      "199998             3                  Low           532                 2   \n",
      "199999             0                 High          1755                 3   \n",
      "\n",
      "        Premium/Amount Ratio Claim Request output  BMI Churn  \n",
      "0                   0.078640                   No   21   Yes  \n",
      "1                   0.100069                   No   24   Yes  \n",
      "2                   0.136533                   No   18   Yes  \n",
      "3                   0.124267                   No   24   Yes  \n",
      "4                   0.127467                   No   21   Yes  \n",
      "...                      ...                  ...  ...   ...  \n",
      "199995              0.108617                   No   18   Yes  \n",
      "199996              0.093259                   No   22   Yes  \n",
      "199997              0.158304                   No   19   Yes  \n",
      "199998              0.110972                   No   24   Yes  \n",
      "199999              0.121960                   No   22   Yes  \n",
      "\n",
      "[200000 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Encoding Categorical Variables\n",
    "# Identify categorical columns in your dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Load your dataset\n",
    "data = pd.read_csv('randomdata.csv')\n",
    "# Identify categorical columns in your dataset\n",
    "categorical_columns = ['Company Name', 'Claim Reason', 'Category Premium']\n",
    "# Perform label encoding for categorical columns\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "# Display the resulting DataFrame\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53283142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Numerical Features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[['Claim Amount', 'Premium/Amount Ratio', 'BMI']] = scaler.fit_transform(data[['Claim Amount', 'Premium/Amount Ratio', 'BMI']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dabd8d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (160000, 12)\n",
      "Testing set shape: (40000, 12)\n"
     ]
    }
   ],
   "source": [
    "# Data Splitting\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data['Churn'], test_size=0.2, random_state=42)\n",
    "# Print the shape of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eefaa418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0    Customer Name  \\\n",
      "0                0  Christine Payne   \n",
      "1                1   Tony Fernandez   \n",
      "2                2  Christopher Kim   \n",
      "3                3     Nicole Allen   \n",
      "4                4       Linda Cruz   \n",
      "...            ...              ...   \n",
      "199995      199995  Matthew Estrada   \n",
      "199996      199996       James Bean   \n",
      "199997      199997      David Meyer   \n",
      "199998      199998     Martha Stone   \n",
      "199999      199999    Shannon Lewis   \n",
      "\n",
      "                                         Customer_Address  \\\n",
      "0       7627 Anderson Rest Apt. 265,Lake Heather, DC 3...   \n",
      "1       3953 Cindy Brook Apt. 147,East Lindatown, TN 4...   \n",
      "2             8693 Walters Mountains,South Tony, TX 88407   \n",
      "3                56926 Webster Coves,Shawnmouth, NV 04853   \n",
      "4           489 Thomas Forges Apt. 305,Jesseton, GA 36765   \n",
      "...                                                   ...   \n",
      "199995       2024 Lopez Gateway,Lake Pamelafort, MS 35772   \n",
      "199996             0268 Lori Falls,West Jeffrey, SC 49142   \n",
      "199997         00573 Miller Cliff,New Allenbury, SC 68902   \n",
      "199998          62681 Peters Cove,South Anthony, RI 99783   \n",
      "199999                    Unit 6569 Box 2236,DPO AE 88045   \n",
      "\n",
      "                         Company Name Claim Reason Data confidentiality  \\\n",
      "0       Williams, Henderson and Perez       Travel                  Low   \n",
      "1                       Moore-Goodwin      Medical                 High   \n",
      "2                        Smith-Holmes        Phone               Medium   \n",
      "3                       Harrell-Perez        Phone               Medium   \n",
      "4          Simpson, Kramer and Hughes        Phone               Medium   \n",
      "...                               ...          ...                  ...   \n",
      "199995               Carlson-Matthews      Medical                 High   \n",
      "199996               Trevino-Cardenas      Medical                 High   \n",
      "199997                    Simon-Evans      Medical                 High   \n",
      "199998       Baker, Brooks and Porter       Travel                  Low   \n",
      "199999        Roth, Merritt and Grant      Medical                 High   \n",
      "\n",
      "        Claim Amount  Category Premium  Premium/Amount Ratio  \\\n",
      "0           0.163621              4794              0.309973   \n",
      "1           0.626197             14390              0.397222   \n",
      "2           0.110966              1875              0.545682   \n",
      "3           0.100957              1875              0.495739   \n",
      "4           0.103568              1875              0.508767   \n",
      "...              ...               ...                   ...   \n",
      "199995      0.679721             14390              0.432023   \n",
      "199996      0.583551             14390              0.369494   \n",
      "199997      0.990862             14390              0.634321   \n",
      "199998      0.231070              4794              0.441611   \n",
      "199999      0.763272             14390              0.486346   \n",
      "\n",
      "       Claim Request output  BMI Churn  \n",
      "0                        No  0.3   Yes  \n",
      "1                        No  0.6   Yes  \n",
      "2                        No  0.0   Yes  \n",
      "3                        No  0.6   Yes  \n",
      "4                        No  0.3   Yes  \n",
      "...                     ...  ...   ...  \n",
      "199995                   No  0.0   Yes  \n",
      "199996                   No  0.4   Yes  \n",
      "199997                   No  0.1   Yes  \n",
      "199998                   No  0.6   Yes  \n",
      "199999                   No  0.4   Yes  \n",
      "\n",
      "[200000 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Load your dataset\n",
    "\n",
    "data = pd.read_csv('randomdata.csv')\n",
    "# Identify numerical columns in your dataset\n",
    "numerical_columns = ['Claim Amount', 'Premium/Amount Ratio', 'BMI']\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "# Fit the MinMaxScaler to the numerical columns\n",
    "scaler.fit(data[numerical_columns])\n",
    "# Transform the numerical columns using the MinMaxScaler\n",
    "data[numerical_columns] = scaler.transform(data[numerical_columns])\n",
    "# Display the resulting DataFrame\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6601ca",
   "metadata": {},
   "source": [
    "DATA SPLITTING 80 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5571e32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (160000, 11)\n",
      "X_test shape: (40000, 11)\n",
      "y_train shape: (160000,)\n",
      "y_test shape: (40000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset\n",
    "\n",
    "data = pd.read_csv('randomdata.csv')\n",
    "\n",
    "# Assuming 'Churn' is your target variable and you want to predict it\n",
    "X = data.drop(columns=['Churn'])  # Features\n",
    "y = data['Churn']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the sizes of the resulting sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec16e4e",
   "metadata": {},
   "source": [
    "DATA SPLITTING 75 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "822f3aa6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (150000, 11)\n",
      "X_test shape: (50000, 11)\n",
      "y_train shape: (150000,)\n",
      "y_test shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset\n",
    "\n",
    "data = pd.read_csv('randomdata.csv')\n",
    "\n",
    "# Assuming 'Churn' is your target variable and you want to predict it\n",
    "X = data.drop(columns=['Churn'])  # Features\n",
    "y = data['Churn']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (75% training, 25% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Display the sizes of the resulting sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0e6ee",
   "metadata": {},
   "source": [
    "DATA SPLITTING 85 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69efabc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (170000, 11)\n",
      "X_test shape: (30000, 11)\n",
      "y_train shape: (170000,)\n",
      "y_test shape: (30000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset\n",
    "# Replace 'randomdata.csv' with the actual path to your dataset\n",
    "data = pd.read_csv('randomdata.csv')\n",
    "\n",
    "# Assuming 'Churn' is your target variable and you want to predict it\n",
    "X = data.drop(columns=['Churn'])  # Features\n",
    "y = data['Churn']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (85% training, 15% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Display the sizes of the resulting sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24031d",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION - Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81843c3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('randomdata.csv')\n",
    "\n",
    "# Assuming 'Churn' is your target variable and you want to predict it\n",
    "# Drop non-numeric columns and target variable\n",
    "X = data.drop(columns=['Churn', 'Customer Name', 'Customer_Address'])\n",
    "y = data['Churn']  # Target variable\n",
    "\n",
    "# Encode categorical variables using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the classifier to your data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the testing data\n",
    "accuracy = rf_classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec897d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6986\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.28      0.41     14536\n",
      "         Yes       0.70      0.94      0.80     25464\n",
      "\n",
      "    accuracy                           0.70     40000\n",
      "   macro avg       0.71      0.61      0.60     40000\n",
      "weighted avg       0.70      0.70      0.66     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "data = pd.read_csv('randomdata.csv')\n",
    "\n",
    "# Assuming 'Churn' is your target variable and you want to predict it\n",
    "# Drop non-numeric columns and target variable\n",
    "X = data.drop(columns=['Churn', 'Customer Name', 'Customer_Address'])\n",
    "y = data['Churn']  # Target variable\n",
    "\n",
    "# Encode categorical variables using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform oversampling on the training data\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize a Deep Learning model (MLPClassifier is used here as an example)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the classifier to your resampled training data\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_result = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc953a0f",
   "metadata": {},
   "source": [
    "# DEEP LEARNING MODEL - Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df4f9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a deep learning model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78708171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create three deep learning models\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential  # Import the Sequential class\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "model1 = create_model(X_train.shape[1])\n",
    "model2 = create_model(X_train.shape[1])\n",
    "model3 = create_model(X_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdcc50ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x209c4001190>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target variable into numerical values (0 and 1)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Train the deep learning models\n",
    "model1.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "model2.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "model3.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "779a09ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 650us/step\n",
      "1250/1250 [==============================] - 1s 643us/step\n",
      "1250/1250 [==============================] - 1s 634us/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the individual models\n",
    "pred1 = model1.predict(X_test)\n",
    "pred2 = model2.predict(X_test)\n",
    "pred3 = model3.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "874de82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble the predictions using a simple averaging method\n",
    "ensemble_preds = np.round((pred1 + pred2 + pred3) / 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10284d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.7327\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy of the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_preds)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef3c105c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 623us/step\n",
      "1250/1250 [==============================] - 1s 634us/step\n",
      "1250/1250 [==============================] - 1s 627us/step\n",
      "Ensemble Model Metrics:\n",
      "Accuracy: 0.7327\n",
      "Precision: 0.7121316560399793\n",
      "F1 Score: 0.8226336219767095\n",
      "Recall: 0.973727615457116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "# Make predictions using the individual models\n",
    "pred1 = model1.predict(X_test)\n",
    "pred2 = model2.predict(X_test)\n",
    "pred3 = model3.predict(X_test)\n",
    "\n",
    "# Ensemble the predictions using Voting Classifier\n",
    "ensemble_preds = np.round((pred1 + pred2 + pred3) / 3)\n",
    "\n",
    "# Calculate classification metrics\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_preds)\n",
    "ensemble_precision = precision_score(y_test, ensemble_preds)\n",
    "ensemble_f1 = f1_score(y_test, ensemble_preds)\n",
    "ensemble_recall = recall_score(y_test, ensemble_preds)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Ensemble Model Metrics:\")\n",
    "print(\"Accuracy:\", ensemble_accuracy)\n",
    "print(\"Precision:\", ensemble_precision)\n",
    "print(\"F1 Score:\", ensemble_f1)\n",
    "print(\"Recall:\", ensemble_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6ba99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f5d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b57837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4d9614c",
   "metadata": {},
   "source": [
    "Experimenting with another ensemble method - bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85a6d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neha\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Ensemble Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "# Create a BaggingClassifier with a RandomForest base estimator\n",
    "bagging_classifier = BaggingClassifier(\n",
    "    base_estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    n_estimators=10, random_state=42)\n",
    "\n",
    "# Fit the bagging ensemble to the data\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the bagging ensemble\n",
    "bagging_accuracy = bagging_classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Bagging Ensemble Accuracy:\", bagging_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbec604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db30aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb0003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67440587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a2f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e8cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105de23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
